{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3f95a3-186d-46f4-bcdc-ea33e58e219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import inspect\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005487e4-0954-467b-b537-b512cb2245e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-head self-attention layer for sequences of shape (B, T, hidden_dim).\n",
    "    Outputs the same shape (B, T, hidden_dim).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.scale = math.sqrt(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, hidden_dim)\n",
    "        returns: (batch, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        Q = self.Wq(x)                            # (B, T, hidden_dim)\n",
    "        K = self.Wk(x)                            # (B, T, hidden_dim)\n",
    "        V = self.Wv(x)                            # (B, T, hidden_dim)\n",
    "\n",
    "        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale  # (B, T, T)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)           # (B, T, T)\n",
    "\n",
    "        out = torch.bmm(attn_weights, V)                             # (B, T, hidden_dim)\n",
    "        return out\n",
    "\n",
    "class LSTMAttnNoDiff(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline LSTM + single-head self-attention for direct forecasting:\n",
    "    - Input:  (B, 192, 1)   (the 'history')\n",
    "    - Output: (B, 192, 1)   (the 'forecast' for the next 48 hours)\n",
    "    \n",
    "    Notes:\n",
    "    - This is a simplified approach: effectively, the model sees 192 input time steps\n",
    "      and produces 192 output steps in one shot.\n",
    "    - There's no \"difference vector\" or \"support\" logic in this baseline.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, 192, 1) representing the 'history' chunk\n",
    "        returns: (batch, 192, 1) as the 'forecast'\n",
    "        \"\"\"\n",
    "        lstm_out, (hn, cn) = self.lstm(x)    # (B, 192, hidden_dim)\n",
    "        attn_out = self.attention(lstm_out)  # (B, 192, hidden_dim)\n",
    "        pred = self.fc_out(attn_out)         # (B, 192, 1)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f66c7d-5163-423b-8a7d-80e4c56cbf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (198071, 384)\n",
      "val_dataset shape: (84914, 384)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = np.load(\"../DatasetCreation/training_dataset.npy\")   # shape: (N, 384)\n",
    "val_dataset   = np.load(\"../DatasetCreation/validation_dataset.npy\") # shape: (M, 384)\n",
    "\n",
    "print(\"train_dataset shape:\", training_dataset.shape)\n",
    "print(\"val_dataset shape:\",   val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be4518a-a575-4b64-a7b8-151a88745c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size):\n",
    "    \"\"\"\n",
    "    dataset: shape (N, 384), each row is a 4-day chunk\n",
    "    returns:\n",
    "      x_tensor: (B, 192, 1) -> model input\n",
    "      y_tensor: (B, 192, 1) -> ground-truth forecast\n",
    "    \"\"\"\n",
    "    idxs = np.random.choice(len(dataset), batch_size, replace=False)\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in idxs:\n",
    "        chunk = dataset[i]         # shape (384,)\n",
    "        x_ = chunk[:192]           # shape (192,) -> model input\n",
    "        y_ = chunk[192:]           # shape (192,) -> ground-truth next 48 hrs\n",
    "        x_list.append(x_)\n",
    "        y_list.append(y_)\n",
    "\n",
    "    x_arr = np.array(x_list)       # (B, 192)\n",
    "    y_arr = np.array(y_list)       # (B, 192)\n",
    "\n",
    "    # Add final dimension\n",
    "    x_tensor = torch.tensor(x_arr, dtype=torch.float).unsqueeze(-1)  # (B, 192, 1)\n",
    "    y_tensor = torch.tensor(y_arr, dtype=torch.float).unsqueeze(-1)  # (B, 192, 1)\n",
    "    return x_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9900a1-e618-4714-a08d-e4f30d740b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000 - MAE: 771.6584, MSE: 953664.0162\n",
      "Epoch 2/10000 - MAE: 630.2396, MSE: 693901.1706\n",
      "Epoch 3/10000 - MAE: 517.7472, MSE: 449423.8153\n",
      "Epoch 4/10000 - MAE: 417.4274, MSE: 281088.7684\n",
      "Epoch 5/10000 - MAE: 283.2042, MSE: 155913.9914\n",
      "Epoch 6/10000 - MAE: 184.6049, MSE: 79255.1807\n",
      "Epoch 7/10000 - MAE: 177.2317, MSE: 76978.0163\n",
      "Epoch 8/10000 - MAE: 177.5070, MSE: 77588.7728\n",
      "Epoch 9/10000 - MAE: 175.0283, MSE: 75269.7239\n",
      "Epoch 10/10000 - MAE: 172.2946, MSE: 74270.7884\n",
      "Epoch 11/10000 - MAE: 171.0407, MSE: 73436.9364\n",
      "Epoch 12/10000 - MAE: 172.3592, MSE: 74980.3309\n",
      "Epoch 13/10000 - MAE: 165.2692, MSE: 69834.3669\n",
      "Epoch 14/10000 - MAE: 172.5609, MSE: 75087.3074\n",
      "Epoch 15/10000 - MAE: 166.0646, MSE: 69406.5939\n",
      "Epoch 16/10000 - MAE: 165.1807, MSE: 67958.7837\n",
      "Epoch 17/10000 - MAE: 163.4492, MSE: 66270.2712\n",
      "Epoch 18/10000 - MAE: 156.3641, MSE: 62764.5245\n",
      "Epoch 19/10000 - MAE: 163.8532, MSE: 66613.2888\n",
      "Epoch 20/10000 - MAE: 161.2004, MSE: 64688.8641\n",
      "Epoch 21/10000 - MAE: 158.4964, MSE: 65501.2589\n",
      "Epoch 22/10000 - MAE: 155.4222, MSE: 62550.0854\n",
      "Epoch 23/10000 - MAE: 155.3300, MSE: 61511.9392\n",
      "Epoch 24/10000 - MAE: 157.7320, MSE: 63930.3290\n",
      "Epoch 25/10000 - MAE: 156.1947, MSE: 62718.4782\n",
      "Epoch 26/10000 - MAE: 155.0344, MSE: 61723.7571\n",
      "Epoch 27/10000 - MAE: 162.4831, MSE: 67076.3945\n",
      "Epoch 28/10000 - MAE: 156.8929, MSE: 62110.8337\n",
      "Epoch 29/10000 - MAE: 159.7602, MSE: 65247.6564\n",
      "Epoch 30/10000 - MAE: 153.2517, MSE: 61064.2839\n",
      "Epoch 31/10000 - MAE: 153.3442, MSE: 60942.7621\n",
      "Epoch 32/10000 - MAE: 157.9711, MSE: 64767.2461\n",
      "Epoch 33/10000 - MAE: 152.9766, MSE: 60842.7827\n",
      "Epoch 34/10000 - MAE: 161.5076, MSE: 66585.7691\n",
      "Epoch 35/10000 - MAE: 154.3955, MSE: 61614.0615\n",
      "Epoch 36/10000 - MAE: 148.6262, MSE: 58505.2487\n",
      "Epoch 37/10000 - MAE: 153.5516, MSE: 62159.3048\n",
      "Epoch 38/10000 - MAE: 155.5270, MSE: 62480.0925\n",
      "Epoch 39/10000 - MAE: 152.4367, MSE: 60524.9558\n",
      "Epoch 40/10000 - MAE: 156.3195, MSE: 63159.2943\n",
      "Epoch 41/10000 - MAE: 148.6198, MSE: 58446.8111\n",
      "Epoch 42/10000 - MAE: 152.4962, MSE: 60615.7526\n",
      "Epoch 43/10000 - MAE: 151.6975, MSE: 60038.9394\n",
      "Epoch 44/10000 - MAE: 151.9190, MSE: 61518.1493\n",
      "Epoch 45/10000 - MAE: 152.3048, MSE: 61213.2618\n",
      "Epoch 46/10000 - MAE: 154.4642, MSE: 61771.6323\n",
      "Epoch 47/10000 - MAE: 147.0798, MSE: 56904.0971\n",
      "Epoch 48/10000 - MAE: 156.4133, MSE: 64183.4462\n",
      "Epoch 49/10000 - MAE: 147.2496, MSE: 58393.1204\n",
      "Epoch 50/10000 - MAE: 148.2834, MSE: 58888.3487\n",
      "Epoch 51/10000 - MAE: 148.5053, MSE: 58933.4915\n",
      "Epoch 52/10000 - MAE: 152.3514, MSE: 61738.8716\n",
      "Epoch 53/10000 - MAE: 151.7544, MSE: 60488.4716\n",
      "Epoch 54/10000 - MAE: 149.1982, MSE: 58910.5457\n",
      "Epoch 55/10000 - MAE: 150.0249, MSE: 60686.0953\n",
      "Epoch 56/10000 - MAE: 146.5822, MSE: 57369.8057\n",
      "Epoch 57/10000 - MAE: 152.3672, MSE: 62011.7531\n",
      "Epoch 58/10000 - MAE: 145.9633, MSE: 55978.1947\n",
      "Epoch 59/10000 - MAE: 152.1903, MSE: 62089.1148\n",
      "Epoch 60/10000 - MAE: 147.0819, MSE: 59953.0254\n",
      "Epoch 61/10000 - MAE: 150.9075, MSE: 61217.7051\n",
      "Epoch 62/10000 - MAE: 147.8246, MSE: 58447.7238\n",
      "Epoch 63/10000 - MAE: 157.0367, MSE: 64954.7151\n",
      "Epoch 64/10000 - MAE: 153.2166, MSE: 60434.5239\n",
      "Epoch 65/10000 - MAE: 152.2201, MSE: 61683.8517\n",
      "Epoch 66/10000 - MAE: 149.5962, MSE: 59402.4467\n",
      "Epoch 67/10000 - MAE: 150.0447, MSE: 60002.4395\n",
      "Epoch 68/10000 - MAE: 150.5334, MSE: 60235.9831\n",
      "Epoch 69/10000 - MAE: 147.1297, MSE: 57125.8154\n",
      "Epoch 70/10000 - MAE: 149.0815, MSE: 58086.4984\n",
      "Epoch 71/10000 - MAE: 152.9005, MSE: 64614.2731\n",
      "Epoch 72/10000 - MAE: 151.4674, MSE: 61938.1639\n",
      "Epoch 73/10000 - MAE: 151.6223, MSE: 61682.2748\n",
      "Epoch 74/10000 - MAE: 144.6067, MSE: 56399.1109\n",
      "Epoch 75/10000 - MAE: 150.4582, MSE: 62082.2728\n",
      "Epoch 76/10000 - MAE: 148.8922, MSE: 59741.4226\n",
      "Epoch 77/10000 - MAE: 153.3413, MSE: 62135.5143\n",
      "Epoch 78/10000 - MAE: 150.5489, MSE: 60581.5940\n",
      "Epoch 79/10000 - MAE: 147.1570, MSE: 57485.1186\n",
      "Epoch 80/10000 - MAE: 148.9443, MSE: 58584.2958\n",
      "Epoch 81/10000 - MAE: 147.1441, MSE: 57564.0637\n",
      "Epoch 82/10000 - MAE: 148.3953, MSE: 59257.6054\n",
      "Epoch 83/10000 - MAE: 151.0194, MSE: 60061.6449\n",
      "Epoch 84/10000 - MAE: 148.8553, MSE: 59815.7768\n",
      "Epoch 85/10000 - MAE: 151.2026, MSE: 61961.4669\n",
      "Epoch 86/10000 - MAE: 151.3631, MSE: 61087.8139\n",
      "Epoch 87/10000 - MAE: 150.6983, MSE: 60262.0503\n",
      "Epoch 88/10000 - MAE: 151.2569, MSE: 61963.1959\n",
      "Epoch 89/10000 - MAE: 149.0897, MSE: 59743.6160\n",
      "Epoch 90/10000 - MAE: 143.0130, MSE: 55745.8391\n",
      "Epoch 91/10000 - MAE: 149.8743, MSE: 59937.3238\n",
      "Epoch 92/10000 - MAE: 149.9673, MSE: 60701.0869\n",
      "Epoch 93/10000 - MAE: 151.9287, MSE: 62933.8234\n",
      "Epoch 94/10000 - MAE: 150.2570, MSE: 59785.5560\n",
      "Epoch 95/10000 - MAE: 149.8003, MSE: 60450.8755\n",
      "Epoch 96/10000 - MAE: 152.6694, MSE: 62302.7282\n",
      "Epoch 97/10000 - MAE: 148.8880, MSE: 58658.7621\n",
      "Epoch 98/10000 - MAE: 147.4187, MSE: 59270.7024\n",
      "Epoch 99/10000 - MAE: 145.9638, MSE: 58701.4712\n",
      "Epoch 100/10000 - MAE: 148.0426, MSE: 58640.7873\n",
      "Epoch 101/10000 - MAE: 150.4463, MSE: 60018.3272\n",
      "Epoch 102/10000 - MAE: 146.7896, MSE: 58595.0166\n",
      "Epoch 103/10000 - MAE: 150.3170, MSE: 60719.6967\n",
      "Epoch 104/10000 - MAE: 148.3254, MSE: 59206.3219\n",
      "Epoch 105/10000 - MAE: 151.3834, MSE: 61273.7410\n",
      "Epoch 106/10000 - MAE: 149.9010, MSE: 61044.9132\n",
      "Epoch 107/10000 - MAE: 148.9923, MSE: 59550.7304\n",
      "Epoch 108/10000 - MAE: 148.6355, MSE: 59618.6906\n",
      "Epoch 109/10000 - MAE: 148.2513, MSE: 59074.1060\n",
      "Epoch 110/10000 - MAE: 147.3569, MSE: 58958.3492\n",
      "Epoch 111/10000 - MAE: 148.3650, MSE: 58525.4944\n",
      "Epoch 112/10000 - MAE: 147.1548, MSE: 58826.8164\n",
      "Epoch 113/10000 - MAE: 149.3242, MSE: 59144.7866\n",
      "Epoch 114/10000 - MAE: 148.3323, MSE: 59457.7583\n",
      "Epoch 115/10000 - MAE: 145.2393, MSE: 57202.4492\n",
      "Epoch 116/10000 - MAE: 146.6504, MSE: 59584.4156\n",
      "Epoch 117/10000 - MAE: 146.9581, MSE: 59899.1510\n",
      "Epoch 118/10000 - MAE: 153.1995, MSE: 63063.2812\n",
      "Epoch 119/10000 - MAE: 149.8047, MSE: 60990.5688\n",
      "Epoch 120/10000 - MAE: 146.8900, MSE: 58074.3399\n",
      "Epoch 121/10000 - MAE: 146.4651, MSE: 58736.3453\n",
      "Epoch 122/10000 - MAE: 150.9738, MSE: 61241.2340\n",
      "Epoch 123/10000 - MAE: 145.0496, MSE: 58145.0764\n",
      "Epoch 124/10000 - MAE: 147.3196, MSE: 58492.5831\n",
      "Epoch 125/10000 - MAE: 146.8030, MSE: 57511.6931\n",
      "Epoch 126/10000 - MAE: 141.5000, MSE: 54106.0647\n",
      "Epoch 127/10000 - MAE: 153.4668, MSE: 62645.6371\n",
      "Epoch 128/10000 - MAE: 146.5204, MSE: 58509.3071\n",
      "Epoch 129/10000 - MAE: 149.5600, MSE: 60696.8074\n",
      "Epoch 130/10000 - MAE: 148.5241, MSE: 59524.4192\n",
      "Epoch 131/10000 - MAE: 151.0076, MSE: 61588.3751\n",
      "Epoch 132/10000 - MAE: 148.0851, MSE: 59442.7436\n",
      "Epoch 133/10000 - MAE: 147.6337, MSE: 59422.3153\n",
      "Epoch 134/10000 - MAE: 144.6877, MSE: 57462.6236\n",
      "Epoch 135/10000 - MAE: 150.8370, MSE: 61429.9036\n",
      "Epoch 136/10000 - MAE: 145.2377, MSE: 57108.4404\n",
      "Epoch 137/10000 - MAE: 149.0879, MSE: 59958.7686\n",
      "Epoch 138/10000 - MAE: 148.7692, MSE: 61434.3725\n",
      "Epoch 139/10000 - MAE: 146.7767, MSE: 58169.7133\n",
      "Epoch 140/10000 - MAE: 151.0662, MSE: 61705.7608\n",
      "Epoch 141/10000 - MAE: 145.4948, MSE: 58528.7573\n",
      "Epoch 142/10000 - MAE: 146.7285, MSE: 57899.0613\n",
      "Epoch 143/10000 - MAE: 146.7920, MSE: 59411.0503\n",
      "Epoch 144/10000 - MAE: 151.8428, MSE: 62246.6433\n",
      "Epoch 145/10000 - MAE: 148.2067, MSE: 60545.8118\n",
      "Epoch 146/10000 - MAE: 146.4928, MSE: 57633.0969\n",
      "Epoch 147/10000 - MAE: 142.3369, MSE: 55777.1022\n",
      "Epoch 148/10000 - MAE: 148.4625, MSE: 60111.4977\n",
      "Epoch 149/10000 - MAE: 143.3997, MSE: 55502.4097\n",
      "Epoch 150/10000 - MAE: 148.0700, MSE: 58001.8326\n",
      "Epoch 151/10000 - MAE: 145.7384, MSE: 57517.7531\n",
      "Epoch 152/10000 - MAE: 146.4195, MSE: 58428.4857\n",
      "Epoch 153/10000 - MAE: 145.7675, MSE: 58971.9688\n",
      "Epoch 154/10000 - MAE: 147.9563, MSE: 60157.0804\n",
      "Epoch 155/10000 - MAE: 144.5694, MSE: 56770.2670\n",
      "Epoch 156/10000 - MAE: 151.9849, MSE: 61173.5306\n",
      "Epoch 157/10000 - MAE: 148.8633, MSE: 59794.3576\n",
      "Epoch 158/10000 - MAE: 147.6499, MSE: 57702.6440\n",
      "Epoch 159/10000 - MAE: 146.6121, MSE: 58486.0501\n",
      "Epoch 160/10000 - MAE: 142.6089, MSE: 56237.2656\n",
      "Epoch 161/10000 - MAE: 148.2870, MSE: 60308.4272\n",
      "Epoch 162/10000 - MAE: 145.7417, MSE: 57367.5209\n",
      "Epoch 163/10000 - MAE: 147.9602, MSE: 58644.2941\n",
      "Epoch 164/10000 - MAE: 149.0191, MSE: 60053.7751\n",
      "Epoch 165/10000 - MAE: 144.2818, MSE: 57360.5171\n",
      "Epoch 166/10000 - MAE: 144.6839, MSE: 57066.8376\n",
      "Epoch 167/10000 - MAE: 148.7736, MSE: 60427.9735\n",
      "Epoch 168/10000 - MAE: 146.4161, MSE: 56793.1152\n",
      "Epoch 169/10000 - MAE: 144.5255, MSE: 56329.1730\n",
      "Epoch 170/10000 - MAE: 146.7099, MSE: 58718.7464\n",
      "Epoch 171/10000 - MAE: 146.7732, MSE: 60141.5006\n",
      "Epoch 172/10000 - MAE: 146.8819, MSE: 58546.1722\n",
      "Epoch 173/10000 - MAE: 140.9180, MSE: 55618.9941\n",
      "Epoch 174/10000 - MAE: 147.1716, MSE: 57750.3815\n",
      "Epoch 175/10000 - MAE: 147.2560, MSE: 58535.3153\n",
      "Epoch 176/10000 - MAE: 147.2943, MSE: 58835.9616\n",
      "Epoch 177/10000 - MAE: 149.0287, MSE: 60987.9935\n",
      "Epoch 178/10000 - MAE: 145.1114, MSE: 56290.4336\n",
      "Epoch 179/10000 - MAE: 145.3375, MSE: 57296.0878\n",
      "Epoch 180/10000 - MAE: 147.7176, MSE: 59217.3469\n",
      "Epoch 181/10000 - MAE: 145.8116, MSE: 56843.7817\n",
      "Epoch 182/10000 - MAE: 145.8923, MSE: 58468.3915\n",
      "Epoch 183/10000 - MAE: 147.4855, MSE: 58374.9256\n",
      "Epoch 184/10000 - MAE: 147.7512, MSE: 59003.5805\n",
      "Epoch 185/10000 - MAE: 146.7890, MSE: 58252.8095\n",
      "Epoch 186/10000 - MAE: 146.4564, MSE: 58461.7836\n",
      "Epoch 187/10000 - MAE: 150.4640, MSE: 59813.8146\n",
      "Epoch 188/10000 - MAE: 147.6335, MSE: 59399.4380\n",
      "Epoch 189/10000 - MAE: 152.9036, MSE: 62266.2162\n",
      "Epoch 190/10000 - MAE: 143.6065, MSE: 55365.7693\n",
      "Epoch 191/10000 - MAE: 145.7359, MSE: 56937.1020\n",
      "Epoch 192/10000 - MAE: 137.6745, MSE: 52366.1958\n",
      "Epoch 193/10000 - MAE: 146.3856, MSE: 58212.5637\n",
      "Epoch 194/10000 - MAE: 144.6682, MSE: 57610.7876\n",
      "Epoch 195/10000 - MAE: 146.5588, MSE: 60507.5882\n",
      "Epoch 196/10000 - MAE: 145.3264, MSE: 57725.5420\n",
      "Epoch 197/10000 - MAE: 146.4569, MSE: 59039.6957\n",
      "Epoch 198/10000 - MAE: 145.3738, MSE: 57794.3585\n",
      "Epoch 199/10000 - MAE: 142.5985, MSE: 55820.4480\n",
      "Epoch 200/10000 - MAE: 145.1305, MSE: 57658.1196\n",
      "Epoch 201/10000 - MAE: 146.8835, MSE: 58931.3027\n",
      "Epoch 202/10000 - MAE: 144.0078, MSE: 55956.4578\n",
      "Epoch 203/10000 - MAE: 143.6103, MSE: 56600.5052\n",
      "Epoch 204/10000 - MAE: 143.2381, MSE: 55176.3562\n",
      "Epoch 205/10000 - MAE: 152.6352, MSE: 62600.5952\n",
      "Epoch 206/10000 - MAE: 147.2314, MSE: 58854.1099\n",
      "Epoch 207/10000 - MAE: 144.7020, MSE: 56250.0583\n",
      "Epoch 208/10000 - MAE: 145.0830, MSE: 59013.5502\n",
      "Epoch 209/10000 - MAE: 147.6395, MSE: 59447.1925\n",
      "Epoch 210/10000 - MAE: 143.9028, MSE: 56895.7374\n",
      "Epoch 211/10000 - MAE: 142.7702, MSE: 56154.0305\n",
      "Epoch 212/10000 - MAE: 146.3882, MSE: 56290.6714\n",
      "Epoch 213/10000 - MAE: 144.2800, MSE: 56766.4262\n",
      "Epoch 214/10000 - MAE: 146.7106, MSE: 56954.3005\n",
      "Epoch 215/10000 - MAE: 143.2007, MSE: 55725.0864\n",
      "Epoch 216/10000 - MAE: 142.7847, MSE: 55168.3585\n",
      "Epoch 217/10000 - MAE: 142.9919, MSE: 56910.1764\n",
      "Epoch 218/10000 - MAE: 145.4751, MSE: 57627.0573\n"
     ]
    }
   ],
   "source": [
    "# train_lstm_attn_no_diff.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LSTMAttnNoDiff(hidden_dim=64, num_layers=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion_mae = nn.L1Loss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "csv_filename = \"lstm_attn_no_diff_log.csv\"\n",
    "with open(csv_filename, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"epoch\", \"MAE\", \"MSE\"])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_mae = 0.0\n",
    "    total_mse = 0.0\n",
    "\n",
    "    # We'll define some number of steps per epoch, e.g. 100\n",
    "    for step in range(100):\n",
    "        x_batch, y_batch = get_batch(training_dataset, BATCH_SIZE)\n",
    "        \n",
    "        x_batch = x_batch.to(device)  # (B, 192, 1)\n",
    "        y_batch = y_batch.to(device)  # (B, 192, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)       # => (B, 192, 1)\n",
    "\n",
    "        loss_mae = criterion_mae(y_pred, y_batch)\n",
    "        loss_mse = criterion_mse(y_pred, y_batch)\n",
    "\n",
    "        loss_mae.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_mae += loss_mae.item()\n",
    "        total_mse += loss_mse.item()\n",
    "\n",
    "    avg_mae = total_mae / (step + 1)\n",
    "    avg_mse = total_mse / (step + 1)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - MAE: {avg_mae:.4f}, MSE: {avg_mse:.4f}\")\n",
    "\n",
    "    # Log to CSV\n",
    "    with open(csv_filename, \"a\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_mae, avg_mse])\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), f\"lstm_attn_no_diff_epoch_{epoch+1}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
