{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa3e5a1-1ada-4aca-9dcb-6345ff10362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dtw in /opt/conda/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from dtw) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from dtw) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install dtw\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from dtw import accelerated_dtw\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f94317d-b50c-451d-b530-6c230b311aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing nearest support neighbors for each query sample...\n",
      "Done. neighbor_array shape: (16636,)\n"
     ]
    }
   ],
   "source": [
    "train_query = np.load(\"../DatasetCreation/training_dataset.npy\")    # shape (NQ, 384)\n",
    "train_support = np.load(\"../DatasetCreation/training_support_dataset.npy\")# shape (NS, 384)\n",
    "\n",
    "def precompute_neighbors(query_dataset, support_dataset):\n",
    "    \"\"\"\n",
    "    For each query row in `query_dataset`, find nearest support index in `support_dataset`.\n",
    "    Returns an array neighbor_array of shape (len(query_dataset),),\n",
    "    where neighbor_array[i] is the best support index for query i.\n",
    "    \"\"\"\n",
    "    neighbor_array = np.zeros(len(query_dataset), dtype=np.int64)\n",
    "    # Loop over all query chunks once\n",
    "    for i in range(len(query_dataset)):\n",
    "        query_chunk = query_dataset[i]      # shape (384,)\n",
    "        query_past  = query_chunk[:192]     # first 192 = 'past'\n",
    "        \n",
    "        best_dist = float('inf')\n",
    "        best_idx  = 0\n",
    "        # find the support row with minimal distance\n",
    "        for j, support_chunk in enumerate(support_dataset):\n",
    "            support_past = support_chunk[:192]\n",
    "            dist = np.linalg.norm(query_past - support_past)\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_idx  = j\n",
    "        neighbor_array[i] = best_idx\n",
    "    return neighbor_array\n",
    "\n",
    "print(\"Precomputing nearest support neighbors for each query sample...\")\n",
    "train_query_neighbors = precompute_neighbors(train_query, train_support)\n",
    "print(\"Done. neighbor_array shape:\", train_query_neighbors.shape)\n",
    "\n",
    "class TransformerEmbed(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=128,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len=192, 1)\n",
    "        x = self.input_proj(x)              # => (batch, 192, d_model)\n",
    "        x = self.transformer_encoder(x)     # => (batch, 192, d_model)\n",
    "        x = x.transpose(1, 2)               # => (batch, d_model, 192)\n",
    "        x = self.pool(x)                    # => (batch, d_model, 1)\n",
    "        x = x.squeeze(-1)                   # => (batch, d_model)\n",
    "        return x\n",
    "\n",
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, transformer_model):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        emb1 = self.transformer(x1)  # => (B, d_model)\n",
    "        emb2 = self.transformer(x2)  # => (B, d_model)\n",
    "        diff = emb1 - emb2\n",
    "        return diff\n",
    "\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_dim=1, diff_dim=64, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.diff_to_hidden = nn.Linear(diff_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, support_future, diff_vec):\n",
    "        \"\"\"\n",
    "        support_future: (B, 192, 1)\n",
    "        diff_vec: (B, 64)\n",
    "        => returns (B, 192, 1)\n",
    "        \"\"\"\n",
    "        h0 = self.diff_to_hidden(diff_vec)   # => (B, hidden_dim)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        # reshape to (num_layers=1, B, hidden_dim)\n",
    "        h0 = h0.unsqueeze(0)\n",
    "        c0 = c0.unsqueeze(0)\n",
    "\n",
    "        lstm_out, (hn, cn) = self.lstm(support_future, (h0, c0))\n",
    "        pred = self.fc_out(lstm_out)         # (B, 192, 1)\n",
    "        return pred\n",
    "\n",
    "def get_batch(query_dataset, support_dataset, neighbor_array, batch_size):\n",
    "    \"\"\"\n",
    "    neighbor_array: shape (NQ,). neighbor_array[i] gives the best support index for query i\n",
    "    query_dataset, support_dataset: shape (NQ,384) or (NS,384)\n",
    "    \n",
    "    returns x_test, y_test, x_support, y_support as Tensors\n",
    "    \"\"\"\n",
    "    idxs = np.random.choice(len(query_dataset), batch_size, replace=False)\n",
    "\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    x_support_list = []\n",
    "    y_support_list = []\n",
    "\n",
    "    for idx in idxs:\n",
    "        chunk = query_dataset[idx]       # shape (384,)\n",
    "        test_past   = chunk[:192]       # shape (192,)\n",
    "        test_future = chunk[192:]       # shape (192,)\n",
    "\n",
    "        # get precomputed support index\n",
    "        idx_support = neighbor_array[idx]\n",
    "        support_chunk = support_dataset[idx_support]\n",
    "        support_past   = support_chunk[:192]\n",
    "        support_future = support_chunk[192:]\n",
    "\n",
    "        x_test_list.append(test_past)\n",
    "        y_test_list.append(test_future)\n",
    "        x_support_list.append(support_past)\n",
    "        y_support_list.append(support_future)\n",
    "\n",
    "    # shape => (B,192)\n",
    "    x_test_arr     = np.array(x_test_list,     dtype=np.float32)\n",
    "    y_test_arr     = np.array(y_test_list,     dtype=np.float32)\n",
    "    x_support_arr  = np.array(x_support_list,  dtype=np.float32)\n",
    "    y_support_arr  = np.array(y_support_list,  dtype=np.float32)\n",
    "\n",
    "    # reshape => (B,192,1)\n",
    "    x_test_tensor     = torch.tensor(x_test_arr).unsqueeze(-1)\n",
    "    y_test_tensor     = torch.tensor(y_test_arr).unsqueeze(-1)\n",
    "    x_support_tensor  = torch.tensor(x_support_arr).unsqueeze(-1)\n",
    "    y_support_tensor  = torch.tensor(y_support_arr).unsqueeze(-1)\n",
    "\n",
    "    return x_test_tensor, y_test_tensor, x_support_tensor, y_support_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25abbd9d-2b3b-4136-970d-5d83628fa37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transformer_model = TransformerEmbed(input_dim=1, d_model=64, nhead=4, num_layers=2)\n",
    "siamese_model = SiameseTransformer(transformer_model).to(device)\n",
    "lstm_model = LSTMForecaster(input_dim=1, diff_dim=64, hidden_dim=64, num_layers=1).to(device)\n",
    "\n",
    "params = list(siamese_model.parameters()) + list(lstm_model.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-3)\n",
    "criterion_mae = nn.L1Loss()\n",
    "criterion_mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0cc77-1ee0-47c3-be54-f2caa950ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000 - MAE: 661.8735, MSE: 788106.1366\n",
      "Epoch 2/10000 - MAE: 656.2703, MSE: 774891.1366\n",
      "Epoch 3/10000 - MAE: 642.6663, MSE: 758345.3600\n",
      "Epoch 4/10000 - MAE: 657.7530, MSE: 780873.1188\n",
      "Epoch 5/10000 - MAE: 640.2299, MSE: 750629.2134\n",
      "Epoch 6/10000 - MAE: 629.2547, MSE: 729054.7227\n",
      "Epoch 7/10000 - MAE: 647.9119, MSE: 769170.5094\n",
      "Epoch 8/10000 - MAE: 640.4367, MSE: 748536.0850\n",
      "Epoch 9/10000 - MAE: 622.5015, MSE: 714810.0647\n",
      "Epoch 10/10000 - MAE: 607.8893, MSE: 708918.9622\n",
      "Epoch 11/10000 - MAE: 612.2960, MSE: 707953.6466\n",
      "Epoch 12/10000 - MAE: 617.0614, MSE: 709980.9016\n",
      "Epoch 13/10000 - MAE: 613.6428, MSE: 717712.3931\n",
      "Epoch 14/10000 - MAE: 600.8577, MSE: 688156.2700\n",
      "Epoch 15/10000 - MAE: 615.9095, MSE: 718897.4911\n",
      "Epoch 16/10000 - MAE: 582.9866, MSE: 661370.9469\n",
      "Epoch 17/10000 - MAE: 589.1715, MSE: 674887.3652\n",
      "Epoch 18/10000 - MAE: 598.0990, MSE: 693644.1642\n",
      "Epoch 19/10000 - MAE: 581.9071, MSE: 655440.4878\n",
      "Epoch 20/10000 - MAE: 574.9502, MSE: 647960.0898\n",
      "Epoch 21/10000 - MAE: 578.6321, MSE: 657042.7422\n",
      "Epoch 22/10000 - MAE: 567.2122, MSE: 633396.1031\n",
      "Epoch 23/10000 - MAE: 578.7728, MSE: 656210.6423\n",
      "Epoch 24/10000 - MAE: 561.4091, MSE: 626101.0481\n",
      "Epoch 25/10000 - MAE: 567.5916, MSE: 637215.7231\n",
      "Epoch 26/10000 - MAE: 574.4782, MSE: 654626.6678\n",
      "Epoch 27/10000 - MAE: 556.8509, MSE: 625236.0628\n",
      "Epoch 28/10000 - MAE: 560.8492, MSE: 628201.7312\n",
      "Epoch 29/10000 - MAE: 554.6823, MSE: 621435.2497\n",
      "Epoch 30/10000 - MAE: 529.3194, MSE: 585020.5455\n",
      "Epoch 31/10000 - MAE: 545.7365, MSE: 606050.6503\n",
      "Epoch 32/10000 - MAE: 530.2707, MSE: 591922.7309\n",
      "Epoch 33/10000 - MAE: 529.7433, MSE: 588072.3628\n",
      "Epoch 34/10000 - MAE: 513.4840, MSE: 564387.2125\n",
      "Epoch 35/10000 - MAE: 522.8319, MSE: 578519.0334\n",
      "Epoch 36/10000 - MAE: 520.1892, MSE: 574038.0098\n",
      "Epoch 37/10000 - MAE: 518.0225, MSE: 570529.0677\n",
      "Epoch 38/10000 - MAE: 512.2562, MSE: 551451.5000\n",
      "Epoch 39/10000 - MAE: 498.9683, MSE: 537186.2894\n",
      "Epoch 40/10000 - MAE: 518.9195, MSE: 567491.2003\n",
      "Epoch 41/10000 - MAE: 502.3227, MSE: 543355.0250\n",
      "Epoch 42/10000 - MAE: 484.9885, MSE: 516113.6611\n",
      "Epoch 43/10000 - MAE: 510.6408, MSE: 556782.8791\n",
      "Epoch 44/10000 - MAE: 493.5770, MSE: 527523.7892\n",
      "Epoch 45/10000 - MAE: 485.9197, MSE: 515401.1698\n",
      "Epoch 46/10000 - MAE: 484.5515, MSE: 510523.8683\n",
      "Epoch 47/10000 - MAE: 484.8982, MSE: 514907.5816\n",
      "Epoch 48/10000 - MAE: 481.7711, MSE: 503567.2413\n",
      "Epoch 49/10000 - MAE: 505.1800, MSE: 538744.2359\n",
      "Epoch 50/10000 - MAE: 469.5587, MSE: 488571.7041\n",
      "Epoch 51/10000 - MAE: 479.1181, MSE: 500029.9394\n",
      "Epoch 52/10000 - MAE: 470.9278, MSE: 490678.6056\n",
      "Epoch 53/10000 - MAE: 476.4814, MSE: 500057.2639\n",
      "Epoch 54/10000 - MAE: 465.4056, MSE: 477694.7331\n",
      "Epoch 55/10000 - MAE: 460.7093, MSE: 475110.2256\n",
      "Epoch 56/10000 - MAE: 453.5323, MSE: 459495.8141\n",
      "Epoch 57/10000 - MAE: 465.1334, MSE: 482332.1747\n",
      "Epoch 58/10000 - MAE: 461.9170, MSE: 472699.9387\n",
      "Epoch 59/10000 - MAE: 459.8706, MSE: 471098.8362\n",
      "Epoch 60/10000 - MAE: 459.7374, MSE: 469434.5625\n",
      "Epoch 61/10000 - MAE: 454.9401, MSE: 461452.1059\n",
      "Epoch 62/10000 - MAE: 471.4589, MSE: 486926.1359\n",
      "Epoch 63/10000 - MAE: 427.9685, MSE: 425826.9553\n",
      "Epoch 64/10000 - MAE: 454.3026, MSE: 458765.8609\n",
      "Epoch 65/10000 - MAE: 444.0547, MSE: 446827.0603\n",
      "Epoch 66/10000 - MAE: 454.8339, MSE: 460996.6369\n",
      "Epoch 67/10000 - MAE: 424.9519, MSE: 411523.3484\n",
      "Epoch 68/10000 - MAE: 449.7708, MSE: 456364.2805\n",
      "Epoch 69/10000 - MAE: 433.5461, MSE: 434335.4370\n",
      "Epoch 70/10000 - MAE: 436.2691, MSE: 431250.7041\n",
      "Epoch 71/10000 - MAE: 428.3672, MSE: 417222.5389\n",
      "Epoch 72/10000 - MAE: 447.3613, MSE: 450591.9667\n",
      "Epoch 73/10000 - MAE: 431.1598, MSE: 426985.7177\n",
      "Epoch 74/10000 - MAE: 439.6748, MSE: 437537.5538\n",
      "Epoch 75/10000 - MAE: 437.4637, MSE: 430364.2008\n",
      "Epoch 76/10000 - MAE: 413.7627, MSE: 398652.0038\n",
      "Epoch 77/10000 - MAE: 414.4280, MSE: 400301.6523\n",
      "Epoch 78/10000 - MAE: 418.3168, MSE: 405592.0225\n",
      "Epoch 79/10000 - MAE: 421.3679, MSE: 402178.3491\n",
      "Epoch 80/10000 - MAE: 412.9722, MSE: 397305.8959\n",
      "Epoch 81/10000 - MAE: 394.9535, MSE: 371839.3447\n",
      "Epoch 82/10000 - MAE: 397.4957, MSE: 371743.2612\n",
      "Epoch 83/10000 - MAE: 399.5690, MSE: 378512.0214\n",
      "Epoch 84/10000 - MAE: 402.0464, MSE: 377715.7566\n",
      "Epoch 85/10000 - MAE: 405.6573, MSE: 378270.7025\n",
      "Epoch 86/10000 - MAE: 393.7649, MSE: 367728.9759\n",
      "Epoch 87/10000 - MAE: 386.2425, MSE: 349113.0259\n",
      "Epoch 88/10000 - MAE: 394.9407, MSE: 373509.8191\n",
      "Epoch 89/10000 - MAE: 383.0675, MSE: 348055.8526\n",
      "Epoch 90/10000 - MAE: 385.7485, MSE: 353062.7244\n",
      "Epoch 91/10000 - MAE: 397.2714, MSE: 364155.9617\n",
      "Epoch 92/10000 - MAE: 400.5163, MSE: 369167.3381\n",
      "Epoch 93/10000 - MAE: 376.3275, MSE: 334792.0384\n",
      "Epoch 94/10000 - MAE: 373.5829, MSE: 334280.0191\n",
      "Epoch 95/10000 - MAE: 384.4468, MSE: 346890.5797\n",
      "Epoch 96/10000 - MAE: 379.7823, MSE: 341828.6109\n",
      "Epoch 97/10000 - MAE: 374.6155, MSE: 330031.2523\n",
      "Epoch 98/10000 - MAE: 366.0186, MSE: 321411.1464\n",
      "Epoch 99/10000 - MAE: 363.1670, MSE: 321424.1580\n",
      "Epoch 100/10000 - MAE: 362.9742, MSE: 315290.0361\n",
      "Epoch 101/10000 - MAE: 374.5211, MSE: 328704.8468\n",
      "Epoch 102/10000 - MAE: 361.2905, MSE: 314472.2184\n",
      "Epoch 103/10000 - MAE: 370.2428, MSE: 326818.4038\n",
      "Epoch 104/10000 - MAE: 366.6390, MSE: 316945.9105\n",
      "Epoch 105/10000 - MAE: 344.5724, MSE: 295466.8091\n",
      "Epoch 106/10000 - MAE: 363.5437, MSE: 318297.3526\n",
      "Epoch 107/10000 - MAE: 350.3877, MSE: 298662.1963\n",
      "Epoch 108/10000 - MAE: 352.6663, MSE: 294678.4145\n",
      "Epoch 109/10000 - MAE: 357.4080, MSE: 307809.6707\n",
      "Epoch 110/10000 - MAE: 338.8869, MSE: 280416.5749\n",
      "Epoch 111/10000 - MAE: 355.2713, MSE: 303715.8580\n",
      "Epoch 112/10000 - MAE: 350.4800, MSE: 297416.8867\n",
      "Epoch 113/10000 - MAE: 354.1595, MSE: 298884.1218\n",
      "Epoch 114/10000 - MAE: 333.9238, MSE: 280041.7830\n",
      "Epoch 115/10000 - MAE: 334.2791, MSE: 273188.1578\n",
      "Epoch 116/10000 - MAE: 336.0269, MSE: 275900.2002\n",
      "Epoch 117/10000 - MAE: 344.2107, MSE: 278255.7752\n",
      "Epoch 118/10000 - MAE: 343.9590, MSE: 286712.6271\n",
      "Epoch 119/10000 - MAE: 328.3171, MSE: 265964.4484\n",
      "Epoch 120/10000 - MAE: 329.3749, MSE: 267907.9797\n",
      "Epoch 121/10000 - MAE: 320.9310, MSE: 262996.9595\n",
      "Epoch 122/10000 - MAE: 322.7255, MSE: 267126.6485\n",
      "Epoch 123/10000 - MAE: 328.4259, MSE: 269356.4592\n",
      "Epoch 124/10000 - MAE: 321.7100, MSE: 257889.3838\n",
      "Epoch 125/10000 - MAE: 327.9674, MSE: 266832.3945\n",
      "Epoch 126/10000 - MAE: 312.0281, MSE: 252047.0116\n",
      "Epoch 127/10000 - MAE: 314.8134, MSE: 249159.6528\n",
      "Epoch 128/10000 - MAE: 312.4628, MSE: 252196.0801\n",
      "Epoch 129/10000 - MAE: 311.7111, MSE: 248430.1349\n",
      "Epoch 130/10000 - MAE: 307.5320, MSE: 243107.4906\n",
      "Epoch 131/10000 - MAE: 313.0013, MSE: 244748.0418\n",
      "Epoch 132/10000 - MAE: 308.5090, MSE: 238583.1903\n",
      "Epoch 133/10000 - MAE: 298.5938, MSE: 232338.8460\n",
      "Epoch 134/10000 - MAE: 316.4495, MSE: 246731.7381\n",
      "Epoch 135/10000 - MAE: 300.0725, MSE: 233197.6535\n",
      "Epoch 136/10000 - MAE: 296.0576, MSE: 226107.4238\n",
      "Epoch 137/10000 - MAE: 294.8097, MSE: 227149.0049\n",
      "Epoch 138/10000 - MAE: 305.4112, MSE: 236600.7580\n",
      "Epoch 139/10000 - MAE: 295.8878, MSE: 226174.2959\n",
      "Epoch 140/10000 - MAE: 291.6867, MSE: 219981.7440\n",
      "Epoch 141/10000 - MAE: 285.2651, MSE: 213837.9143\n",
      "Epoch 142/10000 - MAE: 285.2720, MSE: 216285.8574\n",
      "Epoch 143/10000 - MAE: 301.0434, MSE: 225211.6989\n",
      "Epoch 144/10000 - MAE: 283.4847, MSE: 205343.4130\n",
      "Epoch 145/10000 - MAE: 284.5421, MSE: 208529.3926\n",
      "Epoch 146/10000 - MAE: 273.0358, MSE: 199443.7871\n",
      "Epoch 147/10000 - MAE: 300.0318, MSE: 222958.5921\n",
      "Epoch 148/10000 - MAE: 277.7201, MSE: 199835.7098\n",
      "Epoch 149/10000 - MAE: 278.8918, MSE: 206570.5723\n",
      "Epoch 150/10000 - MAE: 275.6397, MSE: 200188.6218\n",
      "Epoch 151/10000 - MAE: 273.8539, MSE: 194117.3712\n",
      "Epoch 152/10000 - MAE: 270.7545, MSE: 191917.6225\n",
      "Epoch 153/10000 - MAE: 272.7059, MSE: 198416.8119\n",
      "Epoch 154/10000 - MAE: 266.2604, MSE: 188257.5593\n",
      "Epoch 155/10000 - MAE: 260.0244, MSE: 178623.7096\n",
      "Epoch 156/10000 - MAE: 270.6392, MSE: 183283.6617\n",
      "Epoch 157/10000 - MAE: 266.6124, MSE: 190318.9136\n",
      "Epoch 158/10000 - MAE: 255.2698, MSE: 174968.6957\n",
      "Epoch 159/10000 - MAE: 250.5362, MSE: 171510.8585\n",
      "Epoch 160/10000 - MAE: 250.6235, MSE: 175519.6188\n",
      "Epoch 161/10000 - MAE: 255.9139, MSE: 176448.0411\n",
      "Epoch 162/10000 - MAE: 250.9164, MSE: 171723.3633\n",
      "Epoch 163/10000 - MAE: 243.9989, MSE: 163166.0657\n",
      "Epoch 164/10000 - MAE: 246.2119, MSE: 168463.7404\n",
      "Epoch 165/10000 - MAE: 251.2616, MSE: 167004.9909\n",
      "Epoch 166/10000 - MAE: 250.4485, MSE: 167937.0467\n",
      "Epoch 167/10000 - MAE: 241.6880, MSE: 159735.6557\n",
      "Epoch 168/10000 - MAE: 253.8709, MSE: 163630.2449\n",
      "Epoch 169/10000 - MAE: 242.6957, MSE: 152786.2745\n",
      "Epoch 170/10000 - MAE: 237.9301, MSE: 153637.2725\n",
      "Epoch 171/10000 - MAE: 240.1298, MSE: 154456.0424\n",
      "Epoch 172/10000 - MAE: 238.9795, MSE: 156579.1193\n",
      "Epoch 173/10000 - MAE: 239.3384, MSE: 157816.8817\n",
      "Epoch 174/10000 - MAE: 225.7651, MSE: 140491.6925\n",
      "Epoch 175/10000 - MAE: 231.7096, MSE: 149338.7995\n",
      "Epoch 176/10000 - MAE: 240.0445, MSE: 156752.0334\n",
      "Epoch 177/10000 - MAE: 240.6366, MSE: 153512.6029\n",
      "Epoch 178/10000 - MAE: 229.0702, MSE: 141628.9314\n",
      "Epoch 179/10000 - MAE: 234.9036, MSE: 149084.3373\n",
      "Epoch 180/10000 - MAE: 227.5446, MSE: 146705.2962\n",
      "Epoch 181/10000 - MAE: 226.3074, MSE: 140987.2361\n",
      "Epoch 182/10000 - MAE: 216.4008, MSE: 131799.5371\n",
      "Epoch 183/10000 - MAE: 221.6718, MSE: 138907.0817\n",
      "Epoch 184/10000 - MAE: 214.1439, MSE: 132535.1303\n",
      "Epoch 185/10000 - MAE: 211.3593, MSE: 128328.1801\n",
      "Epoch 186/10000 - MAE: 213.0864, MSE: 132003.2182\n",
      "Epoch 187/10000 - MAE: 211.9987, MSE: 128525.1725\n",
      "Epoch 188/10000 - MAE: 214.6880, MSE: 129744.0973\n",
      "Epoch 189/10000 - MAE: 204.9779, MSE: 121070.2646\n",
      "Epoch 190/10000 - MAE: 209.6877, MSE: 122180.5135\n",
      "Epoch 191/10000 - MAE: 213.0388, MSE: 128825.6729\n",
      "Epoch 192/10000 - MAE: 202.7345, MSE: 120002.1972\n",
      "Epoch 193/10000 - MAE: 205.1999, MSE: 121417.8883\n",
      "Epoch 194/10000 - MAE: 202.4340, MSE: 119025.6904\n",
      "Epoch 195/10000 - MAE: 201.1345, MSE: 120079.7180\n",
      "Epoch 196/10000 - MAE: 192.7512, MSE: 111461.1268\n",
      "Epoch 197/10000 - MAE: 196.5418, MSE: 111918.0034\n",
      "Epoch 198/10000 - MAE: 321.4879, MSE: 237830.1441\n",
      "Epoch 199/10000 - MAE: 313.0728, MSE: 236866.9492\n",
      "Epoch 200/10000 - MAE: 300.4691, MSE: 225455.9895\n",
      "Epoch 201/10000 - MAE: 305.5781, MSE: 233638.8908\n",
      "Epoch 202/10000 - MAE: 287.6046, MSE: 212254.4022\n",
      "Epoch 203/10000 - MAE: 294.3198, MSE: 223335.9978\n",
      "Epoch 204/10000 - MAE: 281.2627, MSE: 207585.5329\n",
      "Epoch 205/10000 - MAE: 297.5559, MSE: 229011.2828\n",
      "Epoch 206/10000 - MAE: 283.6254, MSE: 207820.1183\n",
      "Epoch 207/10000 - MAE: 290.0112, MSE: 212776.7899\n",
      "Epoch 208/10000 - MAE: 288.1182, MSE: 213225.2963\n",
      "Epoch 209/10000 - MAE: 282.6779, MSE: 211613.6034\n",
      "Epoch 210/10000 - MAE: 280.3023, MSE: 205626.4623\n",
      "Epoch 211/10000 - MAE: 267.8078, MSE: 190582.9005\n",
      "Epoch 212/10000 - MAE: 284.5923, MSE: 203962.8657\n",
      "Epoch 213/10000 - MAE: 309.7779, MSE: 211162.9829\n",
      "Epoch 214/10000 - MAE: 278.2866, MSE: 198978.3803\n",
      "Epoch 215/10000 - MAE: 280.7392, MSE: 202748.3509\n",
      "Epoch 216/10000 - MAE: 271.0134, MSE: 194207.6882\n",
      "Epoch 217/10000 - MAE: 270.9584, MSE: 194626.6592\n",
      "Epoch 218/10000 - MAE: 271.7834, MSE: 194367.0912\n",
      "Epoch 219/10000 - MAE: 262.1433, MSE: 183771.5071\n",
      "Epoch 220/10000 - MAE: 255.2228, MSE: 179668.2457\n",
      "Epoch 221/10000 - MAE: 270.6604, MSE: 193585.2751\n",
      "Epoch 222/10000 - MAE: 248.3866, MSE: 170127.6178\n",
      "Epoch 223/10000 - MAE: 254.8464, MSE: 173494.8227\n",
      "Epoch 224/10000 - MAE: 251.0551, MSE: 169165.1248\n",
      "Epoch 225/10000 - MAE: 254.1540, MSE: 174574.4346\n",
      "Epoch 226/10000 - MAE: 251.2772, MSE: 172603.1301\n",
      "Epoch 227/10000 - MAE: 250.4596, MSE: 173600.8021\n",
      "Epoch 228/10000 - MAE: 251.1634, MSE: 171493.5810\n",
      "Epoch 229/10000 - MAE: 245.0002, MSE: 164194.5039\n",
      "Epoch 230/10000 - MAE: 247.7723, MSE: 167669.8723\n",
      "Epoch 231/10000 - MAE: 245.1353, MSE: 164619.0596\n",
      "Epoch 232/10000 - MAE: 237.1508, MSE: 156545.4987\n",
      "Epoch 233/10000 - MAE: 235.1064, MSE: 157714.7891\n",
      "Epoch 234/10000 - MAE: 238.4595, MSE: 158059.7393\n",
      "Epoch 235/10000 - MAE: 226.3326, MSE: 144583.8312\n",
      "Epoch 236/10000 - MAE: 239.4653, MSE: 160343.0750\n",
      "Epoch 237/10000 - MAE: 234.3834, MSE: 156530.3200\n",
      "Epoch 238/10000 - MAE: 248.6751, MSE: 169481.8296\n",
      "Epoch 239/10000 - MAE: 231.1082, MSE: 150140.7433\n",
      "Epoch 240/10000 - MAE: 225.9588, MSE: 144068.4093\n",
      "Epoch 241/10000 - MAE: 229.4689, MSE: 147403.8786\n",
      "Epoch 242/10000 - MAE: 227.1455, MSE: 145108.8476\n",
      "Epoch 243/10000 - MAE: 227.0599, MSE: 144542.1174\n",
      "Epoch 244/10000 - MAE: 227.1742, MSE: 146227.2796\n",
      "Epoch 245/10000 - MAE: 222.2514, MSE: 141745.2156\n",
      "Epoch 246/10000 - MAE: 231.5005, MSE: 150958.3847\n",
      "Epoch 247/10000 - MAE: 222.6808, MSE: 139795.7412\n",
      "Epoch 248/10000 - MAE: 232.0145, MSE: 146620.1162\n",
      "Epoch 249/10000 - MAE: 221.2811, MSE: 138954.0044\n",
      "Epoch 250/10000 - MAE: 218.2961, MSE: 133224.4230\n",
      "Epoch 251/10000 - MAE: 212.3381, MSE: 130718.2511\n",
      "Epoch 252/10000 - MAE: 218.4302, MSE: 133474.9214\n",
      "Epoch 253/10000 - MAE: 214.2736, MSE: 130032.9658\n",
      "Epoch 254/10000 - MAE: 203.0691, MSE: 119439.1638\n",
      "Epoch 255/10000 - MAE: 207.6309, MSE: 121965.0896\n",
      "Epoch 256/10000 - MAE: 205.6991, MSE: 126593.9743\n",
      "Epoch 257/10000 - MAE: 206.0488, MSE: 122278.8637\n",
      "Epoch 258/10000 - MAE: 199.0854, MSE: 113988.7230\n",
      "Epoch 259/10000 - MAE: 205.4830, MSE: 121502.1842\n",
      "Epoch 260/10000 - MAE: 207.0637, MSE: 123224.7804\n",
      "Epoch 261/10000 - MAE: 200.7414, MSE: 117855.6162\n",
      "Epoch 262/10000 - MAE: 195.0950, MSE: 110673.8516\n",
      "Epoch 263/10000 - MAE: 200.4591, MSE: 117266.6202\n",
      "Epoch 264/10000 - MAE: 195.4620, MSE: 111512.5843\n",
      "Epoch 265/10000 - MAE: 197.7439, MSE: 115219.2557\n",
      "Epoch 266/10000 - MAE: 198.5888, MSE: 117190.6555\n",
      "Epoch 267/10000 - MAE: 186.4371, MSE: 100670.3698\n",
      "Epoch 268/10000 - MAE: 190.3706, MSE: 109523.3420\n",
      "Epoch 269/10000 - MAE: 186.9343, MSE: 104722.8070\n",
      "Epoch 270/10000 - MAE: 195.3335, MSE: 112352.1960\n",
      "Epoch 271/10000 - MAE: 184.5205, MSE: 101134.5217\n",
      "Epoch 272/10000 - MAE: 189.2418, MSE: 104693.3835\n",
      "Epoch 273/10000 - MAE: 185.5559, MSE: 103158.0740\n",
      "Epoch 274/10000 - MAE: 188.6451, MSE: 108036.4423\n",
      "Epoch 275/10000 - MAE: 191.0152, MSE: 107953.8421\n",
      "Epoch 276/10000 - MAE: 178.2867, MSE: 93926.7341\n",
      "Epoch 277/10000 - MAE: 181.7289, MSE: 99969.1727\n",
      "Epoch 278/10000 - MAE: 182.4274, MSE: 99298.4133\n",
      "Epoch 279/10000 - MAE: 181.7271, MSE: 97070.9166\n",
      "Epoch 280/10000 - MAE: 175.5365, MSE: 92335.0294\n",
      "Epoch 281/10000 - MAE: 179.8755, MSE: 96245.9337\n",
      "Epoch 282/10000 - MAE: 181.4084, MSE: 95768.3550\n",
      "Epoch 283/10000 - MAE: 185.6592, MSE: 97471.0601\n",
      "Epoch 284/10000 - MAE: 179.9196, MSE: 93649.2937\n",
      "Epoch 285/10000 - MAE: 175.1836, MSE: 93283.3439\n",
      "Epoch 286/10000 - MAE: 173.7019, MSE: 88196.2683\n",
      "Epoch 287/10000 - MAE: 166.3776, MSE: 84963.6145\n",
      "Epoch 288/10000 - MAE: 170.7951, MSE: 86153.3253\n",
      "Epoch 289/10000 - MAE: 172.5792, MSE: 88136.7682\n",
      "Epoch 290/10000 - MAE: 179.0150, MSE: 93657.8721\n",
      "Epoch 291/10000 - MAE: 162.3595, MSE: 82957.0461\n",
      "Epoch 292/10000 - MAE: 172.6658, MSE: 88817.3927\n",
      "Epoch 293/10000 - MAE: 166.3923, MSE: 80131.8778\n",
      "Epoch 294/10000 - MAE: 164.6612, MSE: 79490.4715\n",
      "Epoch 295/10000 - MAE: 165.0583, MSE: 83043.6066\n",
      "Epoch 296/10000 - MAE: 161.9233, MSE: 79872.2643\n",
      "Epoch 297/10000 - MAE: 155.7664, MSE: 75268.3042\n",
      "Epoch 298/10000 - MAE: 159.8673, MSE: 79264.5945\n",
      "Epoch 299/10000 - MAE: 163.2417, MSE: 79463.3508\n",
      "Epoch 300/10000 - MAE: 156.6169, MSE: 74139.8424\n",
      "Epoch 301/10000 - MAE: 165.2535, MSE: 82655.6755\n",
      "Epoch 302/10000 - MAE: 158.9719, MSE: 77037.3777\n",
      "Epoch 303/10000 - MAE: 157.5727, MSE: 76231.4928\n",
      "Epoch 304/10000 - MAE: 157.1067, MSE: 75218.6951\n",
      "Epoch 305/10000 - MAE: 148.9881, MSE: 69360.0375\n",
      "Epoch 306/10000 - MAE: 157.5719, MSE: 76307.3556\n",
      "Epoch 307/10000 - MAE: 152.2462, MSE: 71221.3640\n",
      "Epoch 308/10000 - MAE: 146.2807, MSE: 66365.4498\n",
      "Epoch 309/10000 - MAE: 147.6849, MSE: 67804.3761\n",
      "Epoch 310/10000 - MAE: 146.7529, MSE: 64311.1771\n",
      "Epoch 311/10000 - MAE: 146.1195, MSE: 65744.2413\n",
      "Epoch 312/10000 - MAE: 147.5035, MSE: 67718.4156\n",
      "Epoch 313/10000 - MAE: 144.6015, MSE: 66160.4810\n",
      "Epoch 314/10000 - MAE: 147.6673, MSE: 67266.8293\n",
      "Epoch 315/10000 - MAE: 147.4988, MSE: 67540.3625\n",
      "Epoch 316/10000 - MAE: 142.3168, MSE: 60666.7218\n",
      "Epoch 317/10000 - MAE: 141.0732, MSE: 61333.4213\n",
      "Epoch 318/10000 - MAE: 142.1290, MSE: 61916.6659\n",
      "Epoch 319/10000 - MAE: 135.2184, MSE: 57244.2697\n",
      "Epoch 320/10000 - MAE: 129.6789, MSE: 51774.3448\n",
      "Epoch 321/10000 - MAE: 136.7294, MSE: 55135.2741\n",
      "Epoch 322/10000 - MAE: 129.8269, MSE: 53111.6573\n",
      "Epoch 323/10000 - MAE: 128.9297, MSE: 51192.9530\n",
      "Epoch 324/10000 - MAE: 134.3396, MSE: 56852.1835\n",
      "Epoch 325/10000 - MAE: 130.5670, MSE: 54941.1388\n",
      "Epoch 326/10000 - MAE: 130.3810, MSE: 51675.7702\n",
      "Epoch 327/10000 - MAE: 130.9625, MSE: 53544.1493\n",
      "Epoch 328/10000 - MAE: 131.4068, MSE: 54006.9175\n",
      "Epoch 329/10000 - MAE: 130.0836, MSE: 53470.6710\n",
      "Epoch 330/10000 - MAE: 134.4557, MSE: 54375.0747\n",
      "Epoch 331/10000 - MAE: 169.5056, MSE: 69845.2552\n",
      "Epoch 332/10000 - MAE: 146.0643, MSE: 59659.6615\n",
      "Epoch 333/10000 - MAE: 143.5518, MSE: 60070.8210\n",
      "Epoch 334/10000 - MAE: 139.1567, MSE: 56659.9230\n",
      "Epoch 335/10000 - MAE: 133.8107, MSE: 53982.8418\n",
      "Epoch 336/10000 - MAE: 132.4864, MSE: 53185.4793\n",
      "Epoch 337/10000 - MAE: 134.1776, MSE: 56726.8580\n",
      "Epoch 338/10000 - MAE: 126.4996, MSE: 50448.8486\n",
      "Epoch 339/10000 - MAE: 129.3988, MSE: 51779.8726\n",
      "Epoch 340/10000 - MAE: 130.3407, MSE: 52287.8475\n",
      "Epoch 341/10000 - MAE: 125.4665, MSE: 47497.7553\n",
      "Epoch 342/10000 - MAE: 129.2330, MSE: 52147.6506\n",
      "Epoch 343/10000 - MAE: 126.8068, MSE: 50785.7528\n",
      "Epoch 344/10000 - MAE: 119.1740, MSE: 44630.8821\n",
      "Epoch 345/10000 - MAE: 120.2877, MSE: 44724.9748\n",
      "Epoch 346/10000 - MAE: 116.0213, MSE: 43186.2999\n",
      "Epoch 347/10000 - MAE: 116.3270, MSE: 43396.3203\n",
      "Epoch 348/10000 - MAE: 118.1792, MSE: 45188.8600\n",
      "Epoch 349/10000 - MAE: 116.7059, MSE: 44346.2069\n",
      "Epoch 350/10000 - MAE: 116.9287, MSE: 43911.8359\n",
      "Epoch 351/10000 - MAE: 116.1050, MSE: 43435.8729\n",
      "Epoch 352/10000 - MAE: 112.7404, MSE: 40765.5846\n",
      "Epoch 353/10000 - MAE: 117.7298, MSE: 44040.0142\n",
      "Epoch 354/10000 - MAE: 115.3081, MSE: 43297.3101\n",
      "Epoch 355/10000 - MAE: 114.1521, MSE: 41919.8138\n",
      "Epoch 356/10000 - MAE: 116.5413, MSE: 43326.4339\n",
      "Epoch 357/10000 - MAE: 113.2752, MSE: 42080.9261\n",
      "Epoch 358/10000 - MAE: 111.9415, MSE: 40564.6059\n",
      "Epoch 359/10000 - MAE: 110.7932, MSE: 38060.7645\n",
      "Epoch 360/10000 - MAE: 110.5587, MSE: 39491.5988\n",
      "Epoch 361/10000 - MAE: 114.8167, MSE: 42500.8294\n",
      "Epoch 362/10000 - MAE: 108.6351, MSE: 38211.7368\n",
      "Epoch 363/10000 - MAE: 108.1352, MSE: 38624.5505\n",
      "Epoch 364/10000 - MAE: 109.8640, MSE: 40076.8202\n",
      "Epoch 365/10000 - MAE: 109.9164, MSE: 40762.4016\n",
      "Epoch 366/10000 - MAE: 110.1941, MSE: 38983.8682\n",
      "Epoch 367/10000 - MAE: 110.2971, MSE: 39225.2916\n",
      "Epoch 368/10000 - MAE: 107.1240, MSE: 39330.7080\n",
      "Epoch 369/10000 - MAE: 103.7143, MSE: 35283.0925\n",
      "Epoch 370/10000 - MAE: 102.3643, MSE: 34096.8032\n",
      "Epoch 371/10000 - MAE: 105.1129, MSE: 35748.3546\n",
      "Epoch 372/10000 - MAE: 109.0549, MSE: 38858.1580\n",
      "Epoch 373/10000 - MAE: 102.6497, MSE: 35806.6128\n",
      "Epoch 374/10000 - MAE: 105.2342, MSE: 37284.2660\n",
      "Epoch 375/10000 - MAE: 105.8370, MSE: 37436.4599\n",
      "Epoch 376/10000 - MAE: 107.0197, MSE: 39011.4200\n",
      "Epoch 377/10000 - MAE: 99.3467, MSE: 32707.0168\n",
      "Epoch 378/10000 - MAE: 105.2444, MSE: 37411.3740\n",
      "Epoch 379/10000 - MAE: 104.0478, MSE: 36091.2933\n",
      "Epoch 380/10000 - MAE: 102.2998, MSE: 34263.3566\n",
      "Epoch 381/10000 - MAE: 101.2295, MSE: 34122.1314\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "csv_filename = 'training_log.csv'\n",
    "with open(csv_filename, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"epoch\", \"MAE\", \"MSE\"])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_mae = 0.0\n",
    "    epoch_mse = 0.0\n",
    "    steps = 100\n",
    "\n",
    "    for step in range(steps):\n",
    "        x_test, y_test, x_support, y_support = get_batch(\n",
    "            query_dataset   = train_query,\n",
    "            support_dataset = train_support,\n",
    "            neighbor_array  = train_query_neighbors,\n",
    "            batch_size      = BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        x_test      = x_test.to(device)\n",
    "        y_test      = y_test.to(device)\n",
    "        x_support   = x_support.to(device)\n",
    "        y_support   = y_support.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # siamese difference\n",
    "        diff_vec = siamese_model(x_test, x_support)   # => (B,64)\n",
    "        # forecast\n",
    "        y_pred = lstm_model(y_support, diff_vec)      # => (B,192,1)\n",
    "\n",
    "        # losses\n",
    "        loss_mae = criterion_mae(y_pred, y_test)\n",
    "        loss_mse = criterion_mse(y_pred, y_test)\n",
    "\n",
    "        # backprop\n",
    "        loss_mae.backward()   # or combine them\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_mae += loss_mae.item()\n",
    "        epoch_mse += loss_mse.item()\n",
    "\n",
    "    avg_mae = epoch_mae / steps\n",
    "    avg_mse = epoch_mse / steps\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - MAE: {avg_mae:.4f}, MSE: {avg_mse:.4f}\")\n",
    "\n",
    "    # log to CSV\n",
    "    with open(csv_filename, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_mae, avg_mse])\n",
    "\n",
    "    # optionally save checkpoints\n",
    "    torch.save(siamese_model.state_dict(), f\"siamese_model_epoch_{epoch+1}.pth\")\n",
    "    torch.save(lstm_model.state_dict(),    f\"lstm_model_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9d35f-8fa0-4551-adf2-282686c44ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
