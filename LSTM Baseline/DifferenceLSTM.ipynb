{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e33d661-6cb8-4eaa-8bf0-e528624bd249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import inspect\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d75474-d2bc-4eff-8c74-4005a0505ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring datasets\n",
      "\n",
      "Done. neighbor_array shape: (198071,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Acquiring datasets\\n\")\n",
    "\n",
    "train_query          = np.load(\"../DatasetCreation/training_dataset.npy\")\n",
    "train_support        = np.load(\"../DatasetCreation/training_support_dataset.npy\")\n",
    "train_query_neighbors= np.load(\"../DatasetCreation/train_query_neighbors.npy\")\n",
    "\n",
    "val_query            = np.load(\"../DatasetCreation/validation_dataset.npy\")\n",
    "val_support          = np.load(\"../DatasetCreation/validation_support_dataset.npy\")\n",
    "val_query_neighbors  = np.load(\"../DatasetCreation/val_query_neighbors.npy\")\n",
    "\n",
    "print(\"Done. neighbor_array shape:\", train_query_neighbors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c11ce34-691d-4c14-9cb0-54e39df54149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal single-head self-attention for sequences:\n",
    "    Input shape:  (B, T, hidden_dim)\n",
    "    Output shape: (B, T, hidden_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.scale = math.sqrt(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, hidden_dim)\n",
    "        returns: (batch_size, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "\n",
    "        attn_scores = torch.bmm(Q, K.transpose(1,2)) / self.scale  # (B, T, T)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)          # (B, T, T)\n",
    "        out = torch.bmm(attn_weights, V)                           # (B, T, hidden_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class FewShotLSTMAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    A direct few-shot baseline:\n",
    "    1) difference = test_past - support_past\n",
    "    2) LSTM hidden init from difference\n",
    "    3) Self-attention on LSTM outputs\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        # We expect difference vector to be shape (B, 192)\n",
    "        # So diff_to_hidden expects input_dim=192\n",
    "        self.diff_to_hidden = nn.Linear(192, hidden_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,         # support_future has shape (..., 1)\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, test_past, support_past, support_future):\n",
    "        \"\"\"\n",
    "        test_past: (B, 192)\n",
    "        support_past: (B, 192)\n",
    "        support_future: (B, 192, 1)\n",
    "        returns: (B, 192, 1)\n",
    "        \"\"\"\n",
    "        # 1) difference\n",
    "        diff_vec = test_past - support_past        # shape (B, 192)\n",
    "\n",
    "        # 2) init hidden/cell from difference\n",
    "        #    linear -> (B, hidden_dim)\n",
    "        h0 = self.diff_to_hidden(diff_vec)         # (B, hidden_dim)\n",
    "        c0 = torch.zeros_like(h0)                  # (B, hidden_dim)\n",
    "\n",
    "        # LSTM expects (num_layers, B, hidden_dim)\n",
    "        h0 = h0.unsqueeze(0)  # => (1, B, hidden_dim)\n",
    "        c0 = c0.unsqueeze(0)  # => (1, B, hidden_dim)\n",
    "\n",
    "        # 3) LSTM on support_future\n",
    "        #    support_future: (B, 192, 1)\n",
    "        lstm_out, (hn, cn) = self.lstm(support_future, (h0, c0))\n",
    "        # => lstm_out: (B, 192, hidden_dim)\n",
    "\n",
    "        # 4) self-attention over the LSTM outputs\n",
    "        attn_out = self.attention(lstm_out)   # => (B, 192, hidden_dim)\n",
    "\n",
    "        # 5) final linear at each time step\n",
    "        pred = self.fc_out(attn_out)         # => (B, 192, 1)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4217d7-dc31-4baf-b3af-0ee69170ad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000 - MAE: 767.7611, MSE: 947067.9475\n",
      "Epoch 2/5000 - MAE: 639.6288, MSE: 702158.1728\n",
      "Epoch 3/5000 - MAE: 513.6589, MSE: 440828.4197\n",
      "Epoch 4/5000 - MAE: 378.7358, MSE: 246965.9924\n",
      "Epoch 5/5000 - MAE: 198.6490, MSE: 88748.9530\n",
      "Epoch 6/5000 - MAE: 127.9689, MSE: 39773.8005\n",
      "Epoch 7/5000 - MAE: 114.7482, MSE: 33059.4392\n",
      "Epoch 8/5000 - MAE: 107.3390, MSE: 26810.4762\n",
      "Epoch 9/5000 - MAE: 109.6244, MSE: 27803.9835\n",
      "Epoch 10/5000 - MAE: 102.8880, MSE: 24663.0698\n",
      "Epoch 11/5000 - MAE: 103.7564, MSE: 25952.9757\n",
      "Epoch 12/5000 - MAE: 106.5233, MSE: 26680.1830\n",
      "Epoch 13/5000 - MAE: 100.2712, MSE: 23901.5317\n",
      "Epoch 14/5000 - MAE: 102.7350, MSE: 25787.1285\n",
      "Epoch 15/5000 - MAE: 104.7859, MSE: 25998.0527\n",
      "Epoch 16/5000 - MAE: 99.5965, MSE: 23354.1059\n",
      "Epoch 17/5000 - MAE: 101.1055, MSE: 23364.3953\n",
      "Epoch 18/5000 - MAE: 102.1044, MSE: 23818.0264\n",
      "Epoch 19/5000 - MAE: 101.4330, MSE: 25895.1333\n",
      "Epoch 20/5000 - MAE: 102.4132, MSE: 24396.6098\n",
      "Epoch 21/5000 - MAE: 102.2964, MSE: 24152.5214\n",
      "Epoch 22/5000 - MAE: 98.5134, MSE: 23659.6469\n",
      "Epoch 23/5000 - MAE: 96.4978, MSE: 21927.2207\n",
      "Epoch 24/5000 - MAE: 97.8327, MSE: 22725.5442\n",
      "Epoch 25/5000 - MAE: 97.5563, MSE: 22124.5318\n",
      "Epoch 26/5000 - MAE: 99.2307, MSE: 23030.4529\n",
      "Epoch 27/5000 - MAE: 96.4214, MSE: 22849.6424\n",
      "Epoch 28/5000 - MAE: 96.0921, MSE: 22419.6739\n",
      "Epoch 29/5000 - MAE: 97.6419, MSE: 21953.3851\n",
      "Epoch 30/5000 - MAE: 95.3489, MSE: 21514.8676\n",
      "Epoch 31/5000 - MAE: 96.1415, MSE: 21704.1640\n",
      "Epoch 32/5000 - MAE: 97.9724, MSE: 23122.6221\n",
      "Epoch 33/5000 - MAE: 96.5780, MSE: 22430.2920\n",
      "Epoch 34/5000 - MAE: 99.7863, MSE: 24811.6727\n",
      "Epoch 35/5000 - MAE: 98.0670, MSE: 22772.8637\n",
      "Epoch 36/5000 - MAE: 98.7248, MSE: 22647.8068\n",
      "Epoch 37/5000 - MAE: 98.9513, MSE: 24004.4247\n",
      "Epoch 38/5000 - MAE: 92.6500, MSE: 19794.5427\n",
      "Epoch 39/5000 - MAE: 95.2457, MSE: 21611.7930\n",
      "Epoch 40/5000 - MAE: 96.3196, MSE: 21682.9963\n",
      "Epoch 41/5000 - MAE: 97.2649, MSE: 23042.9559\n",
      "Epoch 42/5000 - MAE: 92.3932, MSE: 19988.5715\n",
      "Epoch 43/5000 - MAE: 95.3357, MSE: 21632.2306\n",
      "Epoch 44/5000 - MAE: 99.7190, MSE: 23963.3432\n",
      "Epoch 45/5000 - MAE: 98.4230, MSE: 23272.4243\n",
      "Epoch 46/5000 - MAE: 101.7439, MSE: 25591.9461\n",
      "Epoch 47/5000 - MAE: 98.6365, MSE: 23419.3768\n",
      "Epoch 48/5000 - MAE: 99.2813, MSE: 24086.5805\n",
      "Epoch 49/5000 - MAE: 97.1368, MSE: 22018.5715\n",
      "Epoch 50/5000 - MAE: 101.0385, MSE: 25828.0981\n",
      "Epoch 51/5000 - MAE: 95.6511, MSE: 21193.4579\n",
      "Epoch 52/5000 - MAE: 98.2463, MSE: 23118.5885\n",
      "Epoch 53/5000 - MAE: 97.1830, MSE: 23048.7016\n",
      "Epoch 54/5000 - MAE: 96.1326, MSE: 21737.1695\n",
      "Epoch 55/5000 - MAE: 95.1196, MSE: 23151.1658\n",
      "Epoch 56/5000 - MAE: 97.2532, MSE: 22624.1767\n",
      "Epoch 57/5000 - MAE: 96.1657, MSE: 22225.5402\n",
      "Epoch 58/5000 - MAE: 93.7576, MSE: 21449.1310\n",
      "Epoch 59/5000 - MAE: 97.5976, MSE: 23730.5700\n",
      "Epoch 60/5000 - MAE: 96.5187, MSE: 21813.5824\n",
      "Epoch 61/5000 - MAE: 93.9582, MSE: 21010.2213\n",
      "Epoch 62/5000 - MAE: 95.2354, MSE: 22340.6838\n",
      "Epoch 63/5000 - MAE: 96.8312, MSE: 23306.6030\n",
      "Epoch 64/5000 - MAE: 97.9653, MSE: 23395.4766\n",
      "Epoch 65/5000 - MAE: 93.8786, MSE: 21194.7583\n",
      "Epoch 66/5000 - MAE: 91.5959, MSE: 20587.7422\n",
      "Epoch 67/5000 - MAE: 94.5699, MSE: 21275.7494\n",
      "Epoch 68/5000 - MAE: 93.6927, MSE: 20886.3463\n",
      "Epoch 69/5000 - MAE: 94.1463, MSE: 22455.9204\n",
      "Epoch 70/5000 - MAE: 95.4515, MSE: 21924.0080\n",
      "Epoch 71/5000 - MAE: 97.9329, MSE: 22657.1534\n",
      "Epoch 72/5000 - MAE: 95.4405, MSE: 21039.9696\n",
      "Epoch 73/5000 - MAE: 94.4933, MSE: 20856.3751\n",
      "Epoch 74/5000 - MAE: 96.2956, MSE: 22632.9593\n",
      "Epoch 75/5000 - MAE: 95.7538, MSE: 22230.3739\n",
      "Epoch 76/5000 - MAE: 95.7266, MSE: 21219.2714\n",
      "Epoch 77/5000 - MAE: 95.1722, MSE: 22280.9758\n",
      "Epoch 78/5000 - MAE: 96.1150, MSE: 22793.2448\n",
      "Epoch 79/5000 - MAE: 96.6186, MSE: 21345.6825\n",
      "Epoch 80/5000 - MAE: 92.7095, MSE: 22010.6920\n",
      "Epoch 81/5000 - MAE: 92.3467, MSE: 19718.6889\n",
      "Epoch 82/5000 - MAE: 95.6800, MSE: 22122.0020\n",
      "Epoch 83/5000 - MAE: 96.2779, MSE: 22667.7285\n",
      "Epoch 84/5000 - MAE: 92.7381, MSE: 20423.3402\n",
      "Epoch 85/5000 - MAE: 92.0022, MSE: 21088.3492\n",
      "Epoch 86/5000 - MAE: 87.4456, MSE: 18672.5414\n",
      "Epoch 87/5000 - MAE: 94.1597, MSE: 20450.6810\n",
      "Epoch 88/5000 - MAE: 95.0666, MSE: 24260.5931\n",
      "Epoch 89/5000 - MAE: 95.9666, MSE: 21302.4083\n",
      "Epoch 90/5000 - MAE: 95.0943, MSE: 22267.9246\n",
      "Epoch 91/5000 - MAE: 96.1904, MSE: 22012.5878\n",
      "Epoch 92/5000 - MAE: 96.1090, MSE: 23325.9721\n",
      "Epoch 93/5000 - MAE: 94.8371, MSE: 22052.7463\n",
      "Epoch 94/5000 - MAE: 95.8495, MSE: 23649.0132\n",
      "Epoch 95/5000 - MAE: 92.0320, MSE: 20800.6867\n",
      "Epoch 96/5000 - MAE: 90.7142, MSE: 20088.6134\n",
      "Epoch 97/5000 - MAE: 92.2449, MSE: 20204.9092\n",
      "Epoch 98/5000 - MAE: 89.0862, MSE: 19381.1890\n",
      "Epoch 99/5000 - MAE: 95.1863, MSE: 21848.5077\n",
      "Epoch 100/5000 - MAE: 94.7429, MSE: 21225.7656\n",
      "Epoch 101/5000 - MAE: 95.1350, MSE: 23306.7272\n",
      "Epoch 102/5000 - MAE: 92.8325, MSE: 21935.1779\n",
      "Epoch 103/5000 - MAE: 94.2060, MSE: 22246.6026\n",
      "Epoch 104/5000 - MAE: 91.2050, MSE: 20822.1126\n",
      "Epoch 105/5000 - MAE: 94.0452, MSE: 21071.1926\n",
      "Epoch 106/5000 - MAE: 92.0053, MSE: 20471.5325\n",
      "Epoch 107/5000 - MAE: 94.5858, MSE: 22803.9596\n",
      "Epoch 108/5000 - MAE: 94.3439, MSE: 21879.1828\n",
      "Epoch 109/5000 - MAE: 93.8450, MSE: 20724.5298\n",
      "Epoch 110/5000 - MAE: 94.9387, MSE: 20853.8387\n",
      "Epoch 111/5000 - MAE: 93.3322, MSE: 22544.3575\n",
      "Epoch 112/5000 - MAE: 92.0584, MSE: 21120.7212\n",
      "Epoch 113/5000 - MAE: 94.0606, MSE: 21461.1517\n",
      "Epoch 114/5000 - MAE: 89.9389, MSE: 19619.3805\n",
      "Epoch 115/5000 - MAE: 89.1798, MSE: 19087.2665\n",
      "Epoch 116/5000 - MAE: 90.2963, MSE: 19286.3334\n",
      "Epoch 117/5000 - MAE: 91.7454, MSE: 20445.4085\n",
      "Epoch 118/5000 - MAE: 91.6168, MSE: 21093.0823\n",
      "Epoch 119/5000 - MAE: 91.1335, MSE: 19528.3595\n",
      "Epoch 120/5000 - MAE: 94.2665, MSE: 21317.5049\n",
      "Epoch 121/5000 - MAE: 94.4688, MSE: 20807.7925\n",
      "Epoch 122/5000 - MAE: 92.0562, MSE: 19981.7879\n",
      "Epoch 123/5000 - MAE: 90.9182, MSE: 19885.9612\n",
      "Epoch 124/5000 - MAE: 91.7113, MSE: 19825.6627\n",
      "Epoch 125/5000 - MAE: 91.1432, MSE: 19679.8253\n",
      "Epoch 126/5000 - MAE: 89.9877, MSE: 19188.9041\n",
      "Epoch 127/5000 - MAE: 91.3355, MSE: 19595.1990\n",
      "Epoch 128/5000 - MAE: 91.7946, MSE: 21136.1878\n",
      "Epoch 129/5000 - MAE: 90.1744, MSE: 20020.1939\n",
      "Epoch 130/5000 - MAE: 94.9742, MSE: 22395.0888\n",
      "Epoch 131/5000 - MAE: 94.8848, MSE: 22062.5663\n",
      "Epoch 132/5000 - MAE: 93.1336, MSE: 21286.1461\n",
      "Epoch 133/5000 - MAE: 92.5963, MSE: 21265.1225\n",
      "Epoch 134/5000 - MAE: 98.8483, MSE: 24770.0973\n",
      "Epoch 135/5000 - MAE: 91.2571, MSE: 20428.2909\n",
      "Epoch 136/5000 - MAE: 93.2617, MSE: 21081.3267\n",
      "Epoch 137/5000 - MAE: 92.1526, MSE: 20794.4657\n",
      "Epoch 138/5000 - MAE: 92.8912, MSE: 21988.3818\n",
      "Epoch 139/5000 - MAE: 92.4826, MSE: 20477.5909\n",
      "Epoch 140/5000 - MAE: 94.7311, MSE: 22489.1501\n",
      "Epoch 141/5000 - MAE: 95.1184, MSE: 22042.7775\n",
      "Epoch 142/5000 - MAE: 93.5324, MSE: 22037.6119\n",
      "Epoch 143/5000 - MAE: 91.5100, MSE: 20356.0411\n",
      "Epoch 144/5000 - MAE: 94.4499, MSE: 21284.8172\n",
      "Epoch 145/5000 - MAE: 93.9391, MSE: 20504.5188\n",
      "Epoch 146/5000 - MAE: 92.5807, MSE: 20817.3656\n",
      "Epoch 147/5000 - MAE: 92.7400, MSE: 21003.7077\n",
      "Epoch 148/5000 - MAE: 92.4385, MSE: 21183.7955\n",
      "Epoch 149/5000 - MAE: 91.2005, MSE: 20228.5939\n",
      "Epoch 150/5000 - MAE: 91.6392, MSE: 20129.8691\n",
      "Epoch 151/5000 - MAE: 91.9778, MSE: 20813.0508\n",
      "Epoch 152/5000 - MAE: 92.5241, MSE: 22397.2466\n",
      "Epoch 153/5000 - MAE: 91.6689, MSE: 20115.2041\n",
      "Epoch 154/5000 - MAE: 92.4250, MSE: 21366.2883\n",
      "Epoch 155/5000 - MAE: 90.5771, MSE: 19858.1681\n",
      "Epoch 156/5000 - MAE: 92.5235, MSE: 20559.1853\n",
      "Epoch 157/5000 - MAE: 90.5867, MSE: 19459.5845\n",
      "Epoch 158/5000 - MAE: 95.1476, MSE: 21304.4759\n",
      "Epoch 159/5000 - MAE: 92.5858, MSE: 21546.6721\n",
      "Epoch 160/5000 - MAE: 96.4896, MSE: 23110.1882\n",
      "Epoch 161/5000 - MAE: 92.3643, MSE: 20585.7321\n",
      "Epoch 162/5000 - MAE: 89.3131, MSE: 20557.0653\n",
      "Epoch 163/5000 - MAE: 92.3707, MSE: 21425.5525\n",
      "Epoch 164/5000 - MAE: 90.3611, MSE: 19662.8366\n",
      "Epoch 165/5000 - MAE: 91.9358, MSE: 20530.6404\n",
      "Epoch 166/5000 - MAE: 94.9273, MSE: 22705.4870\n",
      "Epoch 167/5000 - MAE: 90.8259, MSE: 19484.4133\n",
      "Epoch 168/5000 - MAE: 93.6332, MSE: 21265.2016\n",
      "Epoch 169/5000 - MAE: 90.2438, MSE: 20046.0206\n",
      "Epoch 170/5000 - MAE: 91.3773, MSE: 20174.5634\n",
      "Epoch 171/5000 - MAE: 92.2536, MSE: 21220.3773\n",
      "Epoch 172/5000 - MAE: 91.3861, MSE: 20056.9936\n",
      "Epoch 173/5000 - MAE: 94.7954, MSE: 21618.3652\n",
      "Epoch 174/5000 - MAE: 90.9372, MSE: 19754.5760\n",
      "Epoch 175/5000 - MAE: 93.9164, MSE: 21537.7566\n",
      "Epoch 176/5000 - MAE: 91.3318, MSE: 20635.9889\n",
      "Epoch 177/5000 - MAE: 90.1389, MSE: 19751.3554\n",
      "Epoch 178/5000 - MAE: 90.8768, MSE: 19664.1738\n",
      "Epoch 179/5000 - MAE: 91.2183, MSE: 20256.7933\n",
      "Epoch 180/5000 - MAE: 91.3278, MSE: 20159.6023\n",
      "Epoch 181/5000 - MAE: 91.1320, MSE: 20463.1421\n",
      "Epoch 182/5000 - MAE: 92.1544, MSE: 20775.4448\n",
      "Epoch 183/5000 - MAE: 93.2031, MSE: 21174.1840\n",
      "Epoch 184/5000 - MAE: 90.9275, MSE: 19731.6410\n",
      "Epoch 185/5000 - MAE: 90.5345, MSE: 19731.4108\n",
      "Epoch 186/5000 - MAE: 93.8155, MSE: 22495.8159\n",
      "Epoch 187/5000 - MAE: 90.3578, MSE: 19638.2119\n",
      "Epoch 188/5000 - MAE: 90.9907, MSE: 20455.6366\n",
      "Epoch 189/5000 - MAE: 88.9014, MSE: 19259.0537\n",
      "Epoch 190/5000 - MAE: 88.7629, MSE: 19008.3049\n",
      "Epoch 191/5000 - MAE: 91.1532, MSE: 20131.2201\n",
      "Epoch 192/5000 - MAE: 90.8026, MSE: 20629.9336\n",
      "Epoch 193/5000 - MAE: 92.6875, MSE: 20132.7722\n",
      "Epoch 194/5000 - MAE: 93.5953, MSE: 21047.0887\n",
      "Epoch 195/5000 - MAE: 89.5718, MSE: 19222.8126\n",
      "Epoch 196/5000 - MAE: 90.6855, MSE: 19614.5218\n",
      "Epoch 197/5000 - MAE: 90.5781, MSE: 20120.1204\n",
      "Epoch 198/5000 - MAE: 89.1034, MSE: 18463.5401\n",
      "Epoch 199/5000 - MAE: 90.2205, MSE: 19017.2593\n",
      "Epoch 200/5000 - MAE: 91.8678, MSE: 20016.6344\n",
      "Epoch 201/5000 - MAE: 89.4664, MSE: 19491.2501\n",
      "Epoch 202/5000 - MAE: 89.8240, MSE: 19342.3157\n",
      "Epoch 203/5000 - MAE: 91.8465, MSE: 21425.1556\n",
      "Epoch 204/5000 - MAE: 93.3846, MSE: 21003.2861\n",
      "Epoch 205/5000 - MAE: 89.5401, MSE: 19147.6233\n",
      "Epoch 206/5000 - MAE: 92.2331, MSE: 19951.9827\n",
      "Epoch 207/5000 - MAE: 87.0985, MSE: 18138.0840\n",
      "Epoch 208/5000 - MAE: 88.7595, MSE: 19612.3011\n",
      "Epoch 209/5000 - MAE: 92.7073, MSE: 21120.6167\n",
      "Epoch 210/5000 - MAE: 92.2840, MSE: 21901.2463\n",
      "Epoch 211/5000 - MAE: 90.9652, MSE: 19625.1759\n",
      "Epoch 212/5000 - MAE: 88.8560, MSE: 19012.0020\n",
      "Epoch 213/5000 - MAE: 90.2807, MSE: 19647.6673\n",
      "Epoch 214/5000 - MAE: 94.4960, MSE: 22235.9628\n",
      "Epoch 215/5000 - MAE: 89.6747, MSE: 19521.0492\n",
      "Epoch 216/5000 - MAE: 90.6944, MSE: 20044.3078\n",
      "Epoch 217/5000 - MAE: 91.4208, MSE: 20759.7203\n",
      "Epoch 218/5000 - MAE: 87.5415, MSE: 18217.0287\n",
      "Epoch 219/5000 - MAE: 91.2714, MSE: 21135.9229\n",
      "Epoch 220/5000 - MAE: 89.0313, MSE: 19060.7022\n",
      "Epoch 221/5000 - MAE: 90.3230, MSE: 20669.7339\n",
      "Epoch 222/5000 - MAE: 90.2469, MSE: 19349.6834\n",
      "Epoch 223/5000 - MAE: 89.4431, MSE: 21002.9608\n",
      "Epoch 224/5000 - MAE: 90.6765, MSE: 19890.8426\n",
      "Epoch 225/5000 - MAE: 90.7540, MSE: 19925.3891\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5000\n",
    "LEARNING_RATE = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = FewShotLSTMAttn(hidden_dim=64, num_layers=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion_mae = nn.L1Loss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "def get_batch(dataset, batch_size, query_neighbors, support_dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_test_tensor: (B, 192) -> test_past\n",
    "      y_test_tensor: (B, 192, 1) -> test_future\n",
    "      x_support_tensor: (B, 192) -> support_past\n",
    "      y_support_tensor: (B, 192, 1) -> support_future\n",
    "    \"\"\"\n",
    "    idxs = np.random.choice(len(dataset), batch_size, replace=False)\n",
    "    \n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    x_support_list = []\n",
    "    y_support_list = []\n",
    "\n",
    "    for i in idxs:\n",
    "        chunk = dataset[i]        # shape (384,)\n",
    "        test_past = chunk[:192]   # (192,)\n",
    "        test_future = chunk[192:] # (192,)\n",
    "\n",
    "        # Use the precomputed nearest support index\n",
    "        nearest_sup_idx = query_neighbors[i]\n",
    "        support_chunk = support_dataset[nearest_sup_idx]  # shape (384,)\n",
    "        support_past   = support_chunk[:192]\n",
    "        support_future = support_chunk[192:]\n",
    "        \n",
    "        x_test_list.append(test_past)\n",
    "        y_test_list.append(test_future)\n",
    "        x_support_list.append(support_past)\n",
    "        y_support_list.append(support_future)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    x_test_arr = np.array(x_test_list, dtype=np.float32)     # (B, 192)\n",
    "    y_test_arr = np.array(y_test_list, dtype=np.float32)     # (B, 192)\n",
    "    x_support_arr = np.array(x_support_list, dtype=np.float32)\n",
    "    y_support_arr = np.array(y_support_list, dtype=np.float32)\n",
    "\n",
    "    # Convert to Tensors, add extra dimension for future\n",
    "    x_test_tensor     = torch.tensor(x_test_arr).to(device)              # (B, 192)\n",
    "    y_test_tensor     = torch.tensor(y_test_arr).unsqueeze(-1).to(device)# (B, 192, 1)\n",
    "    x_support_tensor  = torch.tensor(x_support_arr).to(device)           # (B, 192)\n",
    "    y_support_tensor  = torch.tensor(y_support_arr).unsqueeze(-1).to(device) # (B, 192, 1)\n",
    "\n",
    "    return x_test_tensor, y_test_tensor, x_support_tensor, y_support_tensor\n",
    "csv_filename = \"few_shot_lstm_attn_log.csv\"\n",
    "with open(csv_filename, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"epoch\", \"MAE\", \"MSE\"])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_mae = 0.0\n",
    "    total_mse = 0.0\n",
    "\n",
    "    for step in range(100):\n",
    "        x_test, y_test, x_support, y_support = get_batch(\n",
    "            train_query,          # dataset\n",
    "            BATCH_SIZE,\n",
    "            train_query_neighbors, # pass the neighbor array\n",
    "            train_support          # pass the support dataset\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        y_pred = model(x_test, x_support, y_support)  # (B, 192, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_mae = criterion_mae(y_pred, y_test)\n",
    "        loss_mse = criterion_mse(y_pred, y_test)\n",
    "\n",
    "        # backprop\n",
    "        loss_mae.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_mae += loss_mae.item()\n",
    "        total_mse += loss_mse.item()\n",
    "\n",
    "    avg_mae = total_mae / 100\n",
    "    avg_mse = total_mse / 100\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - MAE: {avg_mae:.4f}, MSE: {avg_mse:.4f}\")\n",
    "\n",
    "    # Log to CSV\n",
    "    with open(csv_filename, \"a\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_mae, avg_mse])\n",
    "\n",
    "    if(epoch >= 2000 or (epoch%100)==0):\n",
    "        torch.save(model.state_dict(), f\"few_shot_lstm_attn_epoch_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c13030-94ba-46e0-8957-91f085112f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
